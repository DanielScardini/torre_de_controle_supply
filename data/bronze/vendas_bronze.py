# Databricks notebook source
"""
Processamento de Vendas - Camada Bronze
=========================================

Este notebook processa dados de vendas online e offline para a camada Bronze,
seguindo o padr√£o Medallion Architecture e as melhores pr√°ticas Python.

## Funcionalidades

- Processamento de vendas online e offline
- Agrega√ß√£o por filial, SKU e data
- Cria√ß√£o de grade completa de vendas
- Salvamento na camada Bronze com metadados
- Timezone S√£o Paulo (GMT-3) para processamento

## Autor

Torre de Controle Supply Chain - 2024
"""

from datetime import datetime, timedelta, date
from typing import Optional, Union
import logging

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pytz import timezone

# =============================================================================
# CONFIGURA√á√ïES GLOBAIS
# =============================================================================

# Nome da tabela de destino na camada Bronze
TABELA_BRONZE_VENDAS: str = "databox.bcg_comum.supply_bronze_vendas_90d_on_off"

# Timezone S√£o Paulo (GMT-3)
TIMEZONE_SP = timezone('America/Sao_Paulo')

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Inicializa√ß√£o do Spark
spark = SparkSession.builder.appName("vendas_bronze").getOrCreate()

# Data de processamento (ontem)
hoje = datetime.now(TIMEZONE_SP) - timedelta(days=1)
hoje_str = hoje.strftime("%Y-%m-%d")
hoje_int = int(hoje.strftime("%Y%m%d"))

logger.info(f"Data de processamento: {hoje}")
logger.info(f"Data string: {hoje_str}")
logger.info(f"Data int: {hoje_int}")
logger.info(f"Timezone: {TIMEZONE_SP}")

# COMMAND ----------

def get_data_inicio(
    hoje: Optional[Union[datetime, date]] = None, 
    dias_retrocesso: int = 90
) -> datetime:
    """
    Retorna datetime de in√≠cio baseado nos √∫ltimos N dias.
    
    Args:
        hoje: Data de refer√™ncia (padr√£o: hoje)
        dias_retrocesso: N√∫mero de dias para retroceder (padr√£o: 90 dias)
        
    Returns:
        datetime: Data de in√≠cio no timezone S√£o Paulo
        
    Raises:
        ValueError: Se dias_retrocesso for negativo
    """
    if dias_retrocesso < 0:
        raise ValueError("dias_retrocesso deve ser positivo")
    
    if hoje is None:
        hoje_dt = datetime.now(TIMEZONE_SP)
    elif isinstance(hoje, datetime):
        hoje_dt = hoje
    else:
        hoje_dt = datetime.combine(hoje, datetime.min.time())
        hoje_dt = TIMEZONE_SP.localize(hoje_dt)
    
    data_inicio = hoje_dt - timedelta(days=dias_retrocesso)
    
    logger.info(f"Data de in√≠cio calculada: {data_inicio}")
    logger.info(f"Dias de retrocesso: {dias_retrocesso}")
    
    return data_inicio

# Executar fun√ß√£o e mostrar resultado
data_inicio = get_data_inicio()
data_inicio_int = int(data_inicio.strftime("%Y%m%d"))
logger.info(f"Data in√≠cio: {data_inicio}")
logger.info(f"Data in√≠cio int: {data_inicio_int}")

# Mostrar DataFrame de exemplo
df_exemplo = spark.range(1).select(
    F.lit(data_inicio).alias("data_inicio"),
    F.lit(data_inicio_int).alias("data_inicio_int"),
    F.lit(90).alias("dias_retrocesso")
)
df_exemplo.show()

# COMMAND ----------

def get_vendas_offline(
    spark_session: SparkSession,
    start_date: int = data_inicio_int,
    end_date: int = hoje_int,
) -> DataFrame:
    """
    Processa vendas offline (loja f√≠sica) da tabela vendafaturadarateada.
    
    Args:
        spark_session: Sess√£o do Spark
        start_date: Data de in√≠cio no formato YYYYMMDD
        end_date: Data de fim no formato YYYYMMDD
        
    Returns:
        DataFrame com vendas offline agregadas por filial, SKU e data
        
    Raises:
        ValueError: Se start_date > end_date
    """
    if start_date > end_date:
        raise ValueError("start_date deve ser menor ou igual a end_date")
    
    logger.info(f"Processando vendas OFFLINE de {start_date} at√© {end_date}")
    
    # Carregar tabela de vendas rateadas (offline)
    df_rateada = (
        spark_session.table("app_venda.vendafaturadarateada")
        .filter(F.col("NmEstadoMercadoria") != '1 - SALDO')
        .filter(F.col("NmTipoNegocio") == 'LOJA FISICA')
        .filter(
            F.col("DtAprovacao").between(start_date, end_date)
            & (F.col("VrOperacao") >= 0)
            & (F.col("VrCustoContabilFilialSku") >= 0)
        )
    )
    
    logger.info(f"Registros rateados carregados: {df_rateada.count()}")
    
    # Carregar tabela de vendas n√£o rateadas para quantidade
    df_nao_rateada = (
        spark_session.table("app_venda.vendafaturadanaorateada")
        .filter(F.col("QtMercadoria") >= 0)
    )
    
    logger.info(f"Registros n√£o rateados carregados: {df_nao_rateada.count()}")
    
    # Unificar dados e aplicar transforma√ß√µes
    df = (
        df_rateada
        .join(df_nao_rateada.select("ChaveFatos", "QtMercadoria"), on="ChaveFatos")
        .withColumn(
            "year_month",
            F.date_format(
                F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
                "yyyyMM"
            ).cast("int")
        )
        .withColumnRenamed("CdFilialVenda", "CdFilial")
        .withColumn(
            "DtAtual",
            F.date_format(
                F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
                "yyyy-MM-dd"
            )
        )
    )
    
    logger.info(f"Registros ap√≥s join: {df.count()}")
    
    # Agregar por filial, SKU e data
    df_agg = (
        df.groupBy(
            "DtAtual",
            "year_month",
            "CdSkuLoja",
            "CdFilial",
        )
        .agg(
            F.sum("VrOperacao").alias("Receita"),
            F.sum("QtMercadoria").alias("QtMercadoria"),
            F.sum("VrCustoContabilFilialSku").alias("Custo")
        )
    )
    
    logger.info(f"Registros ap√≥s agrega√ß√£o: {df_agg.count()}")
    
    # Criar grade completa de datas
    cal = (
        spark_session.range(1)
        .select(
            F.explode(
                F.sequence(
                    F.to_date(F.lit(str(start_date)), "yyyyMMdd"),
                    F.to_date(F.lit(str(end_date)), "yyyyMMdd"),
                    F.expr("interval 1 day")
                )
            ).alias("DtAtual_date")
        )
    )
    
    # Conjunto de chaves (Filial x SKU) para loja f√≠sica
    keys = (
        df
        .select("CdFilial", "CdSkuLoja")
        .dropDuplicates()
    )
    
    logger.info(f"Chaves √∫nicas (Filial x SKU): {keys.count()}")
    
    # Grade completa (Data x Filial x SKU)
    grade = cal.crossJoin(keys)
    
    # Agregado com data como DateType para join
    df_agg_d = df_agg.withColumn("DtAtual_date", F.to_date("DtAtual"))
    
    # Left join + zeros onde n√£o houver venda
    result = (
        grade.join(
            df_agg_d,
            on=["DtAtual_date", "CdSkuLoja", "CdFilial"],
            how="left"
        )
        .withColumn("Receita", F.coalesce(F.col("Receita"), F.lit(0.0)))
        .withColumn("QtMercadoria", F.coalesce(F.col("QtMercadoria"), F.lit(0.0)))
        .withColumn("year_month", F.date_format(F.col("DtAtual_date"), "yyyyMM").cast("int"))
        .withColumn("DtAtual", F.date_format(F.col("DtAtual_date"), "yyyy-MM-dd"))
        .withColumnRenamed("CdSkuLoja", "CdSku")
        .select("DtAtual", "year_month", "CdFilial", "CdSku", "Receita", "QtMercadoria")
        .withColumn(
            "TeveVenda",
            F.when(F.col("QtMercadoria") > 0, F.lit(1)).otherwise(F.lit(0))
        )
        .withColumn("Canal", F.lit("OFFLINE"))
    )
    
    logger.info(f"Registros finais OFFLINE: {result.count()}")
    
    # Mostrar amostra dos dados
    logger.info("Amostra dos dados OFFLINE:")
    result.show(5)
    
    return result

# Executar fun√ß√£o de vendas offline
vendas_offline_df = get_vendas_offline(spark)
logger.info(f"DataFrame vendas offline criado com {vendas_offline_df.count()} registros")

# COMMAND ----------

def get_vendas_online(
    spark_session: SparkSession,
    start_date: int = data_inicio_int,
    end_date: int = hoje_int,
) -> DataFrame:
    """
    Processa vendas online da tabela vendafaturadarateada.
    
    Args:
        spark_session: Sess√£o do Spark
        start_date: Data de in√≠cio no formato YYYYMMDD
        end_date: Data de fim no formato YYYYMMDD
        
    Returns:
        DataFrame com vendas online agregadas por filial, SKU e data
        
    Raises:
        ValueError: Se start_date > end_date
    """
    if start_date > end_date:
        raise ValueError("start_date deve ser menor ou igual a end_date")
    
    logger.info(f"Processando vendas ONLINE de {start_date} at√© {end_date}")
    
    # Carregar tabela de vendas rateadas (online)
    df_rateada = (
        spark_session.table("app_venda.vendafaturadarateada")
        .filter(F.col("NmEstadoMercadoria") != '1 - SALDO')
        .filter(F.col("NmTipoNegocio") != 'LOJA FISICA')  # Excluir loja f√≠sica
        .filter(
            F.col("DtAprovacao").between(start_date, end_date)
            & (F.col("VrOperacao") >= 0)
            & (F.col("VrCustoContabilFilialSku") >= 0)
        )
    )
    
    logger.info(f"Registros rateados ONLINE carregados: {df_rateada.count()}")
    
    # Carregar tabela de vendas n√£o rateadas para quantidade
    df_nao_rateada = (
        spark_session.table("app_venda.vendafaturadanaorateada")
        .filter(F.col("QtMercadoria") >= 0)
    )
    
    # Unificar dados e aplicar transforma√ß√µes
    df = (
        df_rateada
        .join(df_nao_rateada.select("ChaveFatos", "QtMercadoria"), on="ChaveFatos")
        .withColumn(
            "year_month",
            F.date_format(
                F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
                "yyyyMM"
            ).cast("int")
        )
        .withColumnRenamed("CdFilialVenda", "CdFilial")
        .withColumn(
            "DtAtual",
            F.date_format(
                F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
                "yyyy-MM-dd"
            )
        )
    )
    
    logger.info(f"Registros ap√≥s join ONLINE: {df.count()}")
    
    # Agregar por filial, SKU e data
    df_agg = (
        df.groupBy(
            "DtAtual",
            "year_month",
            "CdSkuLoja",
            "CdFilial",
        )
        .agg(
            F.sum("VrOperacao").alias("Receita"),
            F.sum("QtMercadoria").alias("QtMercadoria"),
            F.sum("VrCustoContabilFilialSku").alias("Custo")
        )
    )
    
    logger.info(f"Registros ap√≥s agrega√ß√£o ONLINE: {df_agg.count()}")
    
    # Criar grade completa de datas
    cal = (
        spark_session.range(1)
        .select(
            F.explode(
                F.sequence(
                    F.to_date(F.lit(str(start_date)), "yyyyMMdd"),
                    F.to_date(F.lit(str(end_date)), "yyyyMMdd"),
                    F.expr("interval 1 day")
                )
            ).alias("DtAtual_date")
        )
    )
    
    # Conjunto de chaves (Filial x SKU) para online
    keys = (
        df
        .select("CdFilial", "CdSkuLoja")
        .dropDuplicates()
    )
    
    logger.info(f"Chaves √∫nicas ONLINE (Filial x SKU): {keys.count()}")
    
    # Grade completa (Data x Filial x SKU)
    grade = cal.crossJoin(keys)
    
    # Agregado com data como DateType para join
    df_agg_d = df_agg.withColumn("DtAtual_date", F.to_date("DtAtual"))
    
    # Left join + zeros onde n√£o houver venda
    result = (
        grade.join(
            df_agg_d,
            on=["DtAtual_date", "CdSkuLoja", "CdFilial"],
            how="left"
        )
        .withColumn("Receita", F.coalesce(F.col("Receita"), F.lit(0.0)))
        .withColumn("QtMercadoria", F.coalesce(F.col("QtMercadoria"), F.lit(0.0)))
        .withColumn("year_month", F.date_format(F.col("DtAtual_date"), "yyyyMM").cast("int"))
        .withColumn("DtAtual", F.date_format(F.col("DtAtual_date"), "yyyy-MM-dd"))
        .withColumnRenamed("CdSkuLoja", "CdSku")
        .select("DtAtual", "year_month", "CdFilial", "CdSku", "Receita", "QtMercadoria")
        .withColumn(
            "TeveVenda",
            F.when(F.col("QtMercadoria") > 0, F.lit(1)).otherwise(F.lit(0))
        )
        .withColumn("Canal", F.lit("ONLINE"))
    )
    
    logger.info(f"Registros finais ONLINE: {result.count()}")
    
    # Mostrar amostra dos dados
    logger.info("Amostra dos dados ONLINE:")
    result.show(5)
    
    return result

# Executar fun√ß√£o de vendas online
vendas_online_df = get_vendas_online(spark)
logger.info(f"DataFrame vendas online criado com {vendas_online_df.count()} registros")

# COMMAND ----------

def consolidar_vendas_online_offline(
    vendas_offline_df: DataFrame,
    vendas_online_df: DataFrame
) -> DataFrame:
    """
    Consolida vendas online e offline em um √∫nico DataFrame.
    
    Args:
        vendas_offline_df: DataFrame com vendas offline
        vendas_online_df: DataFrame com vendas online
        
    Returns:
        DataFrame consolidado com vendas de ambos os canais
        
    Raises:
        ValueError: Se os DataFrames estiverem vazios
    """
    if vendas_offline_df.count() == 0 and vendas_online_df.count() == 0:
        raise ValueError("Ambos os DataFrames de vendas est√£o vazios")
    
    logger.info("Consolidando vendas ONLINE e OFFLINE...")
    
    # Unir os DataFrames
    vendas_consolidadas = vendas_offline_df.union(vendas_online_df)
    
    logger.info(f"Total de registros consolidados: {vendas_consolidadas.count()}")
    
    # Mostrar estat√≠sticas por canal
    logger.info("Estat√≠sticas por canal:")
    vendas_consolidadas.groupBy("Canal").agg(
        F.count("*").alias("Total_Registros"),
        F.sum("Receita").alias("Receita_Total"),
        F.sum("QtMercadoria").alias("Quantidade_Total"),
        F.sum("TeveVenda").alias("Dias_Com_Venda")
    ).show()
    
    # Mostrar amostra dos dados consolidados
    logger.info("Amostra dos dados consolidados:")
    vendas_consolidadas.show(10)
    
    return vendas_consolidadas

# Executar consolida√ß√£o
vendas_consolidadas_df = consolidar_vendas_online_offline(vendas_offline_df, vendas_online_df)
logger.info(f"DataFrame consolidado criado com {vendas_consolidadas_df.count()} registros")

# COMMAND ----------

def salvar_tabela_bronze(
    df: DataFrame,
    nome_tabela: str = TABELA_BRONZE_VENDAS,
    modo: str = "overwrite"
) -> bool:
    """
    Salva DataFrame na camada Bronze com metadados de processamento.
    
    Args:
        df: DataFrame para salvar
        nome_tabela: Nome da tabela de destino
        modo: Modo de salvamento (overwrite, append)
        
    Returns:
        bool: True se salvamento foi bem-sucedido
        
    Raises:
        ValueError: Se modo n√£o for v√°lido
    """
    if modo not in ["overwrite", "append"]:
        raise ValueError("modo deve ser 'overwrite' ou 'append'")
    
    try:
        logger.info(f"Salvando tabela {nome_tabela} no modo {modo}...")
        
        # Adicionar metadados de processamento
        df_com_metadados = df.withColumn(
            "DataHoraProcessamento", 
            F.current_timestamp()
        ).withColumn(
            "DataProcessamento",
            F.current_date()
        ).withColumn(
            "FonteDados",
            F.lit("app_venda.vendafaturadarateada + vendafaturadanaorateada")
        ).withColumn(
            "VersaoProcessamento",
            F.lit("1.0")
        )
        
        # Salvar na camada Bronze
        df_com_metadados.write \
            .format("delta") \
            .mode(modo) \
            .option("overwriteSchema", "true") \
            .saveAsTable(nome_tabela)
        
        logger.info(f"‚úÖ Tabela {nome_tabela} salva com sucesso!")
        logger.info(f"Registros salvos: {df_com_metadados.count()}")
        
        # Mostrar schema da tabela salva
        logger.info("Schema da tabela salva:")
        spark.table(nome_tabela).printSchema()
        
        # Mostrar amostra dos dados salvos
        logger.info("Amostra dos dados salvos:")
        spark.table(nome_tabela).show(5)
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao salvar tabela {nome_tabela}: {str(e)}")
        return False

# Executar salvamento
sucesso = salvar_tabela_bronze(vendas_consolidadas_df)
if sucesso:
    logger.info("üéâ Processamento de vendas Bronze conclu√≠do com sucesso!")
else:
    logger.error("üí• Falha no processamento de vendas Bronze!")