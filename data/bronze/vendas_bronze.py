# Databricks notebook source
# MAGIC %md
# MAGIC # Processamento de Vendas - Camada Bronze
# MAGIC
# MAGIC Este notebook processa dados de vendas online e offline para a camada Bronze,
# MAGIC seguindo o padrÃ£o Medallion Architecture e as melhores prÃ¡ticas Python.
# MAGIC
# MAGIC **Author**: Torre de Controle Supply Chain  
# MAGIC **Date**: 2024  
# MAGIC **Purpose**: Processar vendas online e offline para anÃ¡lise de demanda e planejamento de estoque

# COMMAND ----------

# MAGIC %md
# MAGIC ## ConfiguraÃ§Ãµes de Ambiente via Widgets
# MAGIC
# MAGIC Este bloco cria widgets interativos para configurar o ambiente de execuÃ§Ã£o.
# MAGIC Os widgets permitem alterar os parÃ¢metros diretamente na interface do Databricks.

# COMMAND ----------

# Criar widgets para configuraÃ§Ãµes
dbutils.widgets.dropdown("modo_execucao", "TEST", ["TEST", "RUN"], "Modo de ExecuÃ§Ã£o")
dbutils.widgets.dropdown("ambiente_tabela", "DEV", ["DEV", "PROD"], "Ambiente da Tabela")
dbutils.widgets.text("sample_size", "10000", "Tamanho do Sample (apenas para TEST)")

# Obter valores dos widgets
MODO_EXECUCAO = dbutils.widgets.get("modo_execucao")
AMBIENTE_TABELA = dbutils.widgets.get("ambiente_tabela")
SAMPLE_SIZE = int(dbutils.widgets.get("sample_size"))

print(f"ğŸ›ï¸ Widgets configurados:")
print(f"  â€¢ Modo de ExecuÃ§Ã£o: {MODO_EXECUCAO}")
print(f"  â€¢ Ambiente da Tabela: {AMBIENTE_TABELA}")
print(f"  â€¢ Tamanho do Sample: {SAMPLE_SIZE:,}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Imports e ConfiguraÃ§Ã£o Inicial

# COMMAND ----------

from datetime import datetime, timedelta, date
from typing import Optional, Union

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pytz import timezone

# =============================================================================
# CONFIGURAÃ‡Ã•ES GLOBAIS
# =============================================================================

# Nome da tabela de destino na camada Bronze (parametrizado por ambiente)
TABELA_BRONZE_VENDAS: str = f"databox.bcg_comum.supply_{AMBIENTE_TABELA.lower()}_vendas_90d_on_off"

# Timezone SÃ£o Paulo (GMT-3)
TIMEZONE_SP = timezone('America/Sao_Paulo')

# =============================================================================
# CONFIGURAÃ‡Ã•ES DE DESENVOLVIMENTO
# =============================================================================

# Usar samples baseado no modo de execuÃ§Ã£o
USAR_SAMPLES: bool = (MODO_EXECUCAO == "TEST")

# Definir quantidade de dias baseado no modo de execuÃ§Ã£o
DIAS_PROCESSAMENTO: int = 1 if MODO_EXECUCAO == "TEST" else 90

# InicializaÃ§Ã£o do Spark
spark = SparkSession.builder.appName("vendas_bronze").getOrCreate()

# Data de processamento (hoje)
hoje = datetime.now(TIMEZONE_SP)
hoje_str = hoje.strftime("%Y-%m-%d")
hoje_int = int(hoje.strftime("%Y%m%d"))

# Data de inÃ­cio baseada no modo de execuÃ§Ã£o
data_inicio = hoje - timedelta(days=DIAS_PROCESSAMENTO)
data_inicio_str = data_inicio.strftime("%Y-%m-%d")

print(f"ğŸ“… Data de processamento: {hoje}")
print(f"ğŸ“ Data string: {hoje_str}")
print(f"ğŸ”¢ Data int: {hoje_int}")
print(f"ğŸŒ Timezone: {TIMEZONE_SP}")
print(f"ğŸ“Š PerÃ­odo de processamento: {DIAS_PROCESSAMENTO} dias")
print(f"ğŸ“… Data inÃ­cio: {data_inicio_str}")
print(f"ğŸ“… Data fim: {hoje_str}")

# =============================================================================
# CONFIGURAÃ‡Ã•ES DE PROCESSAMENTO
# =============================================================================
print("=" * 80)
print("ğŸ”§ CONFIGURAÃ‡Ã•ES DE PROCESSAMENTO:")
print(f"  â€¢ Modo de ExecuÃ§Ã£o: {MODO_EXECUCAO}")
print(f"  â€¢ Ambiente da Tabela: {AMBIENTE_TABELA}")
print(f"  â€¢ Tabela de Destino: {TABELA_BRONZE_VENDAS}")
print(f"  â€¢ PerÃ­odo de Dados: {DIAS_PROCESSAMENTO} dias")
print(f"  â€¢ Data InÃ­cio: {data_inicio_str}")
print(f"  â€¢ Data Fim: {hoje_str}")
print(f"  â€¢ Usar Samples: {USAR_SAMPLES}")
if USAR_SAMPLES:
    print(f"  â€¢ Tamanho do Sample: {SAMPLE_SIZE:,} registros")
    print("  â€¢ âš ï¸  MODO TEST - Usando samples para desenvolvimento")
else:
    print("  â€¢ âœ… MODO RUN - Processamento completo")
print("=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ConfiguraÃ§Ã£o do PerÃ­odo de Processamento
# MAGIC
# MAGIC Este bloco configura o perÃ­odo de dados baseado no modo de execuÃ§Ã£o:
# MAGIC - **TEST**: 1 dia de dados para desenvolvimento rÃ¡pido
# MAGIC - **RUN**: 90 dias de dados para anÃ¡lise completa

# COMMAND ----------

# Converter data_inicio para formato int (jÃ¡ calculado anteriormente)
data_inicio_int = int(data_inicio.strftime("%Y%m%d"))

print(f"ğŸ“Š Data de inÃ­cio calculada: {data_inicio}")
print(f"â° Dias de processamento: {DIAS_PROCESSAMENTO}")
print(f"ğŸ“… Data inÃ­cio: {data_inicio}")
print(f"ğŸ”¢ Data inÃ­cio int: {data_inicio_int}")

# DataFrame de exemplo para verificaÃ§Ã£o
df_exemplo = spark.range(1).select(
    F.lit(data_inicio).alias("data_inicio"),
    F.lit(data_inicio_int).alias("data_inicio_int"),
    F.lit(DIAS_PROCESSAMENTO).alias("dias_processamento")
)

# display(df_exemplo)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Carregamento de Vendas Offline (Loja FÃ­sica)

# COMMAND ----------

print(f"ğŸª Processando vendas OFFLINE de {data_inicio_int} atÃ© {hoje_int}")

# ğŸ’¾ Cache Strategy: Aplicamos cache nos DataFrames principais que serÃ£o
# reutilizados mÃºltiplas vezes durante o processamento para otimizar performance
# Carregar tabela de vendas rateadas (offline)
vendas_rateadas_offline_df = (
        spark.table("app_venda.vendafaturadarateada")
        .filter(F.col("NmEstadoMercadoria") != '1 - SALDO')
        .filter(F.col("NmTipoNegocio") == 'LOJA FISICA')
        .filter(
        F.col("DtAprovacao").between(data_inicio_int, hoje_int)
            & (F.col("VrOperacao") >= 0)
            & (F.col("VrCustoContabilFilialSku") >= 0)
        )
    )  

# Aplicar sample se configurado para desenvolvimento
if USAR_SAMPLES:
    print(f"ğŸ”¬ Aplicando sample de {SAMPLE_SIZE:,} registros para desenvolvimento...")
    vendas_rateadas_offline_df = vendas_rateadas_offline_df.sample(fraction=0.1, seed=42).limit(SAMPLE_SIZE)

vendas_rateadas_offline_df = vendas_rateadas_offline_df.cache()

print(f"ğŸ“Š Registros rateados carregados: {vendas_rateadas_offline_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Carregamento de Quantidades (Vendas NÃ£o Rateadas)

# COMMAND ----------

# Carregar tabela de vendas nÃ£o rateadas para quantidade
vendas_nao_rateadas_df = (
        spark.table("app_venda.vendafaturadanaorateada")
        .filter(F.col("QtMercadoria") >= 0)
        .filter(F.col("DtEmissaoFaturamento").between(data_inicio_int, hoje_int))
        .select("ChaveFatos", "QtMercadoria")
)

print(f"ğŸ“ˆ Registros nÃ£o rateados carregados: {vendas_nao_rateadas_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## UnificaÃ§Ã£o dos Dados Offline

# COMMAND ----------

# Unificar dados e aplicar transformaÃ§Ãµes
vendas_offline_unificadas_df = (
    vendas_rateadas_offline_df
    .join(vendas_nao_rateadas_df.select("ChaveFatos", "QtMercadoria"), on="ChaveFatos")
        .withColumn(
            "year_month",
        F.date_format(
            F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
            "yyyyMM"
        ).cast("int")
    )
    .withColumnRenamed("CdFilialVenda", "CdFilial")
    .withColumn(
        "DtAtual",
        F.date_format(
            F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
            "yyyy-MM-dd"
        )
    )
).cache()

print(f"ğŸ”— Registros apÃ³s join: {vendas_offline_unificadas_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## AgregaÃ§Ã£o das Vendas Offline

# COMMAND ----------

# Agregar por filial, SKU e data
vendas_offline_agregadas_df = (
    vendas_offline_unificadas_df.groupBy(
            "DtAtual",
            "year_month",
            "CdSkuLoja",
            "CdFilial",
        )
        .agg(
            F.sum("VrOperacao").alias("Receita"),
            F.sum("QtMercadoria").alias("QtMercadoria"),
            F.sum("VrCustoContabilFilialSku").alias("Custo")
        )
    )

print(f"ğŸ“Š Registros apÃ³s agregaÃ§Ã£o: {vendas_offline_agregadas_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## CriaÃ§Ã£o da Grade Completa de Datas

# COMMAND ----------

# Criar grade completa de datas
calendario_df = (
        spark.range(1)
        .select(
            F.explode(
                F.sequence(
                F.to_date(F.lit(str(data_inicio_int)), "yyyyMMdd"),
                F.to_date(F.lit(str(hoje_int)), "yyyyMMdd"),
                    F.expr("interval 1 day")
                )
            ).alias("DtAtual_date")
        )
    )

# Conjunto de chaves (Filial x SKU) para loja fÃ­sica
chaves_filial_sku_df = (
    vendas_offline_unificadas_df
          .select("CdFilial", "CdSkuLoja")
          .dropDuplicates()
    )

print(f"ğŸ”‘ Chaves Ãºnicas (Filial x SKU): {chaves_filial_sku_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Grade Completa Offline (Data x Filial x SKU)

# COMMAND ----------

# Grade completa (Data x Filial x SKU)
grade_completa_offline_df = calendario_df.crossJoin(chaves_filial_sku_df)

# Agregado com data como DateType para join
vendas_offline_agregadas_com_data_df = vendas_offline_agregadas_df.withColumn("DtAtual_date", F.to_date("DtAtual"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## AplicaÃ§Ã£o da Grade Completa Offline

# COMMAND ----------

# Left join + zeros onde nÃ£o houver venda
vendas_offline_final_df = (
    grade_completa_offline_df.join(
        vendas_offline_agregadas_com_data_df,
        on=["DtAtual_date", "CdSkuLoja", "CdFilial"],
            how="left"
    )
    .withColumn("Receita", F.coalesce(F.col("Receita"), F.lit(0.0)))
    .withColumn("QtMercadoria", F.coalesce(F.col("QtMercadoria"), F.lit(0.0)))
    .withColumn("year_month", F.date_format(F.col("DtAtual_date"), "yyyyMM").cast("int"))
    .withColumn("DtAtual", F.date_format(F.col("DtAtual_date"), "yyyy-MM-dd"))
    .withColumnRenamed("CdSkuLoja", "CdSku")
    .select("DtAtual", "year_month", "CdFilial", "CdSku", "Receita", "QtMercadoria")
    .withColumn(
        "TeveVenda",
        F.when(F.col("QtMercadoria") > 0, F.lit(1)).otherwise(F.lit(0))
    )
    .withColumn("Canal", F.lit("OFFLINE"))
)

print(f"âœ… Registros finais OFFLINE: {vendas_offline_final_df.count()}")

# Mostrar amostra dos dados
print("ğŸ“‹ Amostra dos dados OFFLINE:")
# display(vendas_offline_final_df.limit(5))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Carregamento de Vendas Online (Canais Digitais)

# COMMAND ----------

print(f"ğŸŒ Processando vendas ONLINE de {data_inicio_int} atÃ© {hoje_int}")

# ğŸ’¾ Cache Strategy: Reutilizando vendas_nao_rateadas_df jÃ¡ em cache
# Carregar tabela de vendas rateadas (online)
vendas_rateadas_online_df = (
    spark.table("app_venda.vendafaturadarateada")
    .filter(F.col("NmEstadoMercadoria") != '1 - SALDO')
    .filter(F.col("NmTipoNegocio") != 'LOJA FISICA')  # Excluir loja fÃ­sica
    .filter(
        F.col("DtAprovacao").between(data_inicio_int, hoje_int)
        & (F.col("VrOperacao") >= 0)
        & (F.col("VrCustoContabilFilialSku") >= 0)
    )
)

# Aplicar sample se configurado para desenvolvimento
if USAR_SAMPLES:
    print(f"ğŸ”¬ Aplicando sample de {SAMPLE_SIZE:,} registros ONLINE para desenvolvimento...")
    vendas_rateadas_online_df = vendas_rateadas_online_df.sample(fraction=0.1, seed=42).limit(SAMPLE_SIZE)

vendas_rateadas_online_df = vendas_rateadas_online_df.cache()

print(f"ğŸ“Š Registros rateados ONLINE carregados: {vendas_rateadas_online_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## UnificaÃ§Ã£o dos Dados Online

# COMMAND ----------

# Unificar dados e aplicar transformaÃ§Ãµes
vendas_online_unificadas_df = (
    vendas_rateadas_online_df
    .join(vendas_nao_rateadas_df.select("ChaveFatos", "QtMercadoria"), on="ChaveFatos")
    .withColumn(
        "year_month",
        F.date_format(
            F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
            "yyyyMM"
        ).cast("int")
    )
    .withColumnRenamed("CdFilialEmissao", "CdFilial")
    .withColumn(
        "DtAtual",
        F.date_format(
            F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
            "yyyy-MM-dd"
        )
    )
).cache()

print(f"ğŸ”— Registros apÃ³s join ONLINE: {vendas_online_unificadas_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## AgregaÃ§Ã£o das Vendas Online

# COMMAND ----------

# Agregar por filial, SKU e data
vendas_online_agregadas_df = (
    vendas_online_unificadas_df.groupBy(
        "DtAtual",
        "year_month",
        "CdSkuLoja",
        "CdFilial",
    )
    .agg(
        F.sum("VrOperacao").alias("Receita"),
        F.sum("QtMercadoria").alias("QtMercadoria"),
        F.sum("VrCustoContabilFilialSku").alias("Custo")
    )
)

print(f"ğŸ“Š Registros apÃ³s agregaÃ§Ã£o ONLINE: {vendas_online_agregadas_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## CriaÃ§Ã£o da Grade Completa Online

# COMMAND ----------

# Conjunto de chaves (Filial x SKU) para online
chaves_filial_sku_online_df = (
    vendas_online_unificadas_df
    .select("CdFilial", "CdSkuLoja")
    .dropDuplicates()
)

print(f"ğŸ”‘ Chaves Ãºnicas ONLINE (Filial x SKU): {chaves_filial_sku_online_df.count()}")

# Grade completa (Data x Filial x SKU)
grade_completa_online_df = calendario_df.crossJoin(chaves_filial_sku_online_df)

# Agregado com data como DateType para join
vendas_online_agregadas_com_data_df = vendas_online_agregadas_df.withColumn("DtAtual_date", F.to_date("DtAtual"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## AplicaÃ§Ã£o da Grade Completa Online

# COMMAND ----------

# Left join + zeros onde nÃ£o houver venda
vendas_online_final_df = (
    grade_completa_online_df.join(
        vendas_online_agregadas_com_data_df,
        on=["DtAtual_date", "CdSkuLoja", "CdFilial"],
        how="left"
    )
    .withColumn("Receita", F.coalesce(F.col("Receita"), F.lit(0.0)))
        .withColumn("QtMercadoria", F.coalesce(F.col("QtMercadoria"), F.lit(0.0)))
        .withColumn("year_month", F.date_format(F.col("DtAtual_date"), "yyyyMM").cast("int"))
    .withColumn("DtAtual", F.date_format(F.col("DtAtual_date"), "yyyy-MM-dd"))
        .withColumnRenamed("CdSkuLoja", "CdSku")
    .select("DtAtual", "year_month", "CdFilial", "CdSku", "Receita", "QtMercadoria")
    .withColumn(
        "TeveVenda",
        F.when(F.col("QtMercadoria") > 0, F.lit(1)).otherwise(F.lit(0))
    )
    .withColumn("Canal", F.lit("ONLINE"))
)

print(f"âœ… Registros finais ONLINE: {vendas_online_final_df.count()}")

# Mostrar amostra dos dados
print("ğŸ“‹ Amostra dos dados ONLINE:")
# display(vendas_online_final_df.limit(5))

# COMMAND ----------

# MAGIC %md
# MAGIC ## ConsolidaÃ§Ã£o de Vendas Online e Offline com Outer Join

# COMMAND ----------

print("ğŸ”„ Consolidando vendas ONLINE e OFFLINE com outer join...")

# ğŸ’¾ Cache Strategy: DataFrame final consolidado em cache para mÃºltiplas operaÃ§Ãµes
# ValidaÃ§Ã£o de dados
if vendas_offline_final_df.count() == 0 and vendas_online_final_df.count() == 0:
    raise ValueError("Ambos os DataFrames de vendas estÃ£o vazios")

# Fazer outer join para garantir que todas as filiais apareÃ§am em ambos os canais
# Primeiro, vamos preparar os DataFrames para o join
vendas_offline_preparadas_df = (
    vendas_offline_final_df
    .withColumnRenamed("Receita", "Receita_OFF")
    .withColumnRenamed("QtMercadoria", "QtMercadoria_OFF")
    .withColumnRenamed("TeveVenda", "TeveVenda_OFF")
    .drop("Canal")  # Remover coluna Canal pois nÃ£o precisamos mais
)

vendas_online_preparadas_df = (
    vendas_online_final_df
    .withColumnRenamed("Receita", "Receita_ON")
    .withColumnRenamed("QtMercadoria", "QtMercadoria_ON")
    .withColumnRenamed("TeveVenda", "TeveVenda_ON")
    .drop("Canal")  # Remover coluna Canal pois nÃ£o precisamos mais
)

# Outer join para garantir todas as combinaÃ§Ãµes
vendas_consolidadas_temp_df = (
    vendas_offline_preparadas_df
    .join(
        vendas_online_preparadas_df,
        on=["DtAtual", "year_month", "CdFilial", "CdSku"],
        how="outer"
    )
)

# Preencher valores nulos com zeros para garantir dados completos
vendas_consolidadas_df = (
    vendas_consolidadas_temp_df
    .fillna(0, subset=[
        "Receita_OFF", "QtMercadoria_OFF", "TeveVenda_OFF",
        "Receita_ON", "QtMercadoria_ON", "TeveVenda_ON"
    ])
    # Calcular totais consolidados
    .withColumn("Receita", F.col("Receita_OFF") + F.col("Receita_ON"))
    .withColumn("QtMercadoria", F.col("QtMercadoria_OFF") + F.col("QtMercadoria_ON"))
    .withColumn("TeveVenda", F.col("TeveVenda_OFF") + F.col("TeveVenda_ON"))
    # Selecionar colunas finais
    .select(
        "DtAtual", "year_month", "CdFilial", "CdSku",
        "Receita_OFF", "QtMercadoria_OFF", "TeveVenda_OFF",
        "Receita_ON", "QtMercadoria_ON", "TeveVenda_ON",
        "Receita", "QtMercadoria", "TeveVenda"
    )
).cache()

print(f"ğŸ“Š Total de registros consolidados: {vendas_consolidadas_df.count()}")

# Mostrar estatÃ­sticas de filiais por canal
print("ğŸ“ˆ EstatÃ­sticas de filiais por canal:")
filiais_offline = vendas_offline_final_df.select("CdFilial").distinct().count()
filiais_online = vendas_online_final_df.select("CdFilial").distinct().count()
filiais_consolidadas = vendas_consolidadas_df.select("CdFilial").distinct().count()

print(f"  â€¢ Filiais com vendas OFFLINE: {filiais_offline}")
print(f"  â€¢ Filiais com vendas ONLINE: {filiais_online}")
print(f"  â€¢ Filiais consolidadas: {filiais_consolidadas}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## AnÃ¡lise EstatÃ­stica Consolidada

# COMMAND ----------

# Mostrar estatÃ­sticas consolidadas
print("ğŸ“ˆ EstatÃ­sticas consolidadas:")
estatisticas_consolidadas_df = vendas_consolidadas_df.agg(
    F.count("*").alias("Total_Registros"),
    F.sum("Receita").alias("Receita_Total"),
    F.sum("QtMercadoria").alias("Quantidade_Total"),
    F.sum("TeveVenda").alias("Dias_Com_Venda"),
    F.countDistinct("CdFilial").alias("Filiais_Unicas"),
    F.countDistinct("CdSku").alias("SKUs_Unicos"),
    F.sum("Receita_OFF").alias("Receita_OFF"),
    F.sum("Receita_ON").alias("Receita_ON"),
    F.sum("QtMercadoria_OFF").alias("Quantidade_OFF"),
    F.sum("QtMercadoria_ON").alias("Quantidade_ON")
)

# display(estatisticas_consolidadas_df)

# Mostrar estatÃ­sticas por filial (top 10)
print("ğŸ“Š Top 10 filiais por receita total:")
top_filiais_df = (
    vendas_consolidadas_df
    .groupBy("CdFilial")
    .agg(
        F.sum("Receita").alias("Receita_Total"),
        F.sum("Receita_OFF").alias("Receita_OFF"),
        F.sum("Receita_ON").alias("Receita_ON"),
        F.sum("QtMercadoria").alias("Quantidade_Total"),
        F.sum("TeveVenda").alias("Dias_Com_Venda")
    )
    .orderBy(F.desc("Receita_Total"))
    .limit(10)
)

# display(top_filiais_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ## AdiÃ§Ã£o de Metadados de Processamento

# COMMAND ----------

# Adicionar metadados de processamento
vendas_com_metadados_df = vendas_consolidadas_df.withColumn(
    "DataHoraProcessamento", 
    F.current_timestamp()
).withColumn(
    "DataProcessamento",
    F.current_date()
).withColumn(
    "FonteDados",
    F.lit("app_venda.vendafaturadarateada + vendafaturadanaorateada")
).withColumn(
    "VersaoProcessamento",
    F.lit("1.0")
)

print(f"ğŸ“Š Registros com metadados: {vendas_com_metadados_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Salvamento na Camada Bronze

# COMMAND ----------

print(f"ğŸ’¾ Salvando tabela {TABELA_BRONZE_VENDAS} no modo overwrite...")

try:
    # Salvar na camada Bronze
    vendas_com_metadados_df.write \
        .format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .saveAsTable(TABELA_BRONZE_VENDAS)
    
    print(f"âœ… Tabela {TABELA_BRONZE_VENDAS} salva com sucesso!")
    print(f"ğŸ“Š Registros salvos: {vendas_com_metadados_df.count()}")

except Exception as e:
    print(f"âŒ Erro ao salvar tabela {TABELA_BRONZE_VENDAS}: {str(e)}")
    raise
finally:
    # Limpar cache para liberar memÃ³ria
    print("ğŸ§¹ Limpando cache para liberar memÃ³ria...")
    vendas_rateadas_offline_df.unpersist()
    vendas_nao_rateadas_df.unpersist()
    vendas_offline_unificadas_df.unpersist()
    vendas_rateadas_online_df.unpersist()
    vendas_online_unificadas_df.unpersist()
    vendas_consolidadas_df.unpersist()
    print("âœ… Cache limpo com sucesso!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ValidaÃ§Ã£o da Tabela Salva

# COMMAND ----------

# Mostrar schema da tabela salva
print("ğŸ“‹ Schema da tabela salva:")
spark.table(TABELA_BRONZE_VENDAS).printSchema()

# Mostrar amostra dos dados salvos
print("ğŸ“‹ Amostra dos dados salvos:")
# display(spark.table(TABELA_BRONZE_VENDAS).limit(5))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Resumo Final do Processamento

# COMMAND ----------

print("ğŸ‰ Processamento de vendas Bronze concluÃ­do com sucesso!")
print("=" * 80)
print("ğŸ“Š RESUMO DO PROCESSAMENTO:")
print(f"  â€¢ PerÃ­odo processado: {data_inicio_str} atÃ© {hoje_str}")
print(f"  â€¢ Dias de histÃ³rico: {DIAS_PROCESSAMENTO}")
print(f"  â€¢ Registros offline: {vendas_offline_final_df.count():,}")
print(f"  â€¢ Registros online: {vendas_online_final_df.count():,}")
print(f"  â€¢ Total consolidado: {vendas_consolidadas_df.count():,}")
print(f"  â€¢ Filiais Ãºnicas: {filiais_consolidadas}")
print(f"  â€¢ Tabela de destino: {TABELA_BRONZE_VENDAS}")
print("=" * 80)
print("âœ… PROCESSAMENTO CONCLUÃDO COM SUCESSO!")
print("ğŸ”„ Outer join aplicado para garantir todas as combinaÃ§Ãµes filial-SKU")
print("ğŸ”¢ Valores nulos preenchidos com zeros")
print("ğŸ“Š Estrutura: SKU x Loja x Dia com colunas separadas por canal")
print("ğŸ“ˆ Colunas: Receita_OFF, Receita_ON, QtMercadoria_OFF, QtMercadoria_ON, etc.")
