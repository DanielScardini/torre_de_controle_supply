# Databricks notebook source
# MAGIC %md
# MAGIC # Processamento de Vendas - Camada Bronze
# MAGIC
# MAGIC Este notebook processa dados de vendas online e offline para a camada Bronze,
# MAGIC seguindo o padr√£o Medallion Architecture e as melhores pr√°ticas Python.
# MAGIC
# MAGIC **Author**: Torre de Controle Supply Chain  
# MAGIC **Date**: 2024  
# MAGIC **Purpose**: Processar vendas online e offline para an√°lise de demanda e planejamento de estoque

# COMMAND ----------

# MAGIC %md
# MAGIC ## Imports e Configura√ß√£o Inicial

# COMMAND ----------

from datetime import datetime, timedelta, date
from typing import Optional, Union

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pytz import timezone

# =============================================================================
# CONFIGURA√á√ïES GLOBAIS
# =============================================================================

# Nome da tabela de destino na camada Bronze
TABELA_BRONZE_VENDAS: str = "databox.bcg_comum.supply_bronze_vendas_90d_on_off"

# Timezone S√£o Paulo (GMT-3)
TIMEZONE_SP = timezone('America/Sao_Paulo')

# =============================================================================
# CONFIGURA√á√ïES DE DESENVOLVIMENTO
# =============================================================================

# Usar samples para desenvolvimento (evitar gasto de processamento)
USAR_SAMPLES: bool = True  # Alterar para False em produ√ß√£o
SAMPLE_SIZE: int = 10000  # Tamanho do sample para desenvolvimento

# Inicializa√ß√£o do Spark
spark = SparkSession.builder.appName("vendas_bronze").getOrCreate()

# Data de processamento (ontem)
hoje = datetime.now(TIMEZONE_SP) - timedelta(days=0)
hoje_str = hoje.strftime("%Y-%m-%d")
hoje_int = int(hoje.strftime("%Y%m%d"))

print(f"üìÖ Data de processamento: {hoje}")
print(f"üìù Data string: {hoje_str}")
print(f"üî¢ Data int: {hoje_int}")
print(f"üåç Timezone: {TIMEZONE_SP}")

# =============================================================================
# CONFIGURA√á√ïES DE PROCESSAMENTO
# =============================================================================
print("=" * 80)
print("üîß CONFIGURA√á√ïES DE PROCESSAMENTO:")
print(f"  ‚Ä¢ Usar Samples: {USAR_SAMPLES}")
if USAR_SAMPLES:
    print(f"  ‚Ä¢ Tamanho do Sample: {SAMPLE_SIZE:,} registros")
    print("  ‚Ä¢ ‚ö†Ô∏è  MODO DESENVOLVIMENTO - Alterar USAR_SAMPLES=False para produ√ß√£o")
else:
    print("  ‚Ä¢ ‚úÖ MODO PRODU√á√ÉO - Processamento completo")
print("=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## C√°lculo da Data de In√≠cio

# COMMAND ----------

# Configura√ß√£o do per√≠odo de an√°lise
dias_retrocesso = 1

# Valida√ß√£o do par√¢metro
if dias_retrocesso < 0:
    raise ValueError("dias_retrocesso deve ser positivo")

# C√°lculo da data de in√≠cio
hoje_dt = datetime.now(TIMEZONE_SP)
data_inicio = hoje_dt - timedelta(days=dias_retrocesso)
data_inicio_int = int(data_inicio.strftime("%Y%m%d"))
data_inicio_str = data_inicio.strftime("%Y-%m-%d")

print(f"üìä Data de in√≠cio calculada: {data_inicio}")
print(f"‚è∞ Dias de retrocesso: {dias_retrocesso}")
print(f"üìÖ Data in√≠cio: {data_inicio}")
print(f"üî¢ Data in√≠cio int: {data_inicio_int}")

# DataFrame de exemplo para verifica√ß√£o
df_exemplo = spark.range(1).select(
    F.lit(data_inicio).alias("data_inicio"),
    F.lit(data_inicio_int).alias("data_inicio_int"),
    F.lit(dias_retrocesso).alias("dias_retrocesso")
)

# display(df_exemplo)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Carregamento de Vendas Offline (Loja F√≠sica)

# COMMAND ----------

print(f"üè™ Processando vendas OFFLINE de {data_inicio_int} at√© {hoje_int}")

# üíæ Cache Strategy: Aplicamos cache nos DataFrames principais que ser√£o
# reutilizados m√∫ltiplas vezes durante o processamento para otimizar performance
# Carregar tabela de vendas rateadas (offline)
vendas_rateadas_offline_df = (
        spark.table("app_venda.vendafaturadarateada")
        .filter(F.col("NmEstadoMercadoria") != '1 - SALDO')
        .filter(F.col("NmTipoNegocio") == 'LOJA FISICA')
        .filter(
        F.col("DtAprovacao").between(data_inicio_int, hoje_int)
            & (F.col("VrOperacao") >= 0)
            & (F.col("VrCustoContabilFilialSku") >= 0)
        )
    )  

# Aplicar sample se configurado para desenvolvimento
if USAR_SAMPLES:
    print(f"üî¨ Aplicando sample de {SAMPLE_SIZE:,} registros para desenvolvimento...")
    vendas_rateadas_offline_df = vendas_rateadas_offline_df.sample(fraction=0.1, seed=42).limit(SAMPLE_SIZE)

vendas_rateadas_offline_df = vendas_rateadas_offline_df.cache()

print(f"üìä Registros rateados carregados: {vendas_rateadas_offline_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Carregamento de Quantidades (Vendas N√£o Rateadas)

# COMMAND ----------

# Carregar tabela de vendas n√£o rateadas para quantidade
vendas_nao_rateadas_df = (
        spark.table("app_venda.vendafaturadanaorateada")
        .filter(F.col("QtMercadoria") >= 0)
        .filter(F.col("DtEmissaoFaturamento").between(data_inicio_int, hoje_int))
        .select("ChaveFatos", "QtMercadoria")
)

print(f"üìà Registros n√£o rateados carregados: {vendas_nao_rateadas_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Unifica√ß√£o dos Dados Offline

# COMMAND ----------

# Unificar dados e aplicar transforma√ß√µes
vendas_offline_unificadas_df = (
    vendas_rateadas_offline_df
    .join(vendas_nao_rateadas_df.select("ChaveFatos", "QtMercadoria"), on="ChaveFatos")
        .withColumn(
            "year_month",
        F.date_format(
            F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
            "yyyyMM"
        ).cast("int")
    )
    .withColumnRenamed("CdFilialVenda", "CdFilial")
    .withColumn(
        "DtAtual",
        F.date_format(
            F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
            "yyyy-MM-dd"
        )
    )
).cache()

print(f"üîó Registros ap√≥s join: {vendas_offline_unificadas_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Agrega√ß√£o das Vendas Offline

# COMMAND ----------

# Agregar por filial, SKU e data
vendas_offline_agregadas_df = (
    vendas_offline_unificadas_df.groupBy(
            "DtAtual",
            "year_month",
            "CdSkuLoja",
            "CdFilial",
        )
        .agg(
            F.sum("VrOperacao").alias("Receita"),
            F.sum("QtMercadoria").alias("QtMercadoria"),
            F.sum("VrCustoContabilFilialSku").alias("Custo")
        )
    )

print(f"üìä Registros ap√≥s agrega√ß√£o: {vendas_offline_agregadas_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Cria√ß√£o da Grade Completa de Datas

# COMMAND ----------

# Criar grade completa de datas
calendario_df = (
        spark.range(1)
        .select(
            F.explode(
                F.sequence(
                F.to_date(F.lit(str(data_inicio_int)), "yyyyMMdd"),
                F.to_date(F.lit(str(hoje_int)), "yyyyMMdd"),
                    F.expr("interval 1 day")
                )
            ).alias("DtAtual_date")
        )
    )

# Conjunto de chaves (Filial x SKU) para loja f√≠sica
chaves_filial_sku_df = (
    vendas_offline_unificadas_df
          .select("CdFilial", "CdSkuLoja")
          .dropDuplicates()
    )

print(f"üîë Chaves √∫nicas (Filial x SKU): {chaves_filial_sku_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Grade Completa Offline (Data x Filial x SKU)

# COMMAND ----------

# Grade completa (Data x Filial x SKU)
grade_completa_offline_df = calendario_df.crossJoin(chaves_filial_sku_df)

# Agregado com data como DateType para join
vendas_offline_agregadas_com_data_df = vendas_offline_agregadas_df.withColumn("DtAtual_date", F.to_date("DtAtual"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Aplica√ß√£o da Grade Completa Offline

# COMMAND ----------

# Left join + zeros onde n√£o houver venda
vendas_offline_final_df = (
    grade_completa_offline_df.join(
        vendas_offline_agregadas_com_data_df,
        on=["DtAtual_date", "CdSkuLoja", "CdFilial"],
            how="left"
    )
    .withColumn("Receita", F.coalesce(F.col("Receita"), F.lit(0.0)))
    .withColumn("QtMercadoria", F.coalesce(F.col("QtMercadoria"), F.lit(0.0)))
    .withColumn("year_month", F.date_format(F.col("DtAtual_date"), "yyyyMM").cast("int"))
    .withColumn("DtAtual", F.date_format(F.col("DtAtual_date"), "yyyy-MM-dd"))
    .withColumnRenamed("CdSkuLoja", "CdSku")
    .select("DtAtual", "year_month", "CdFilial", "CdSku", "Receita", "QtMercadoria")
    .withColumn(
        "TeveVenda",
        F.when(F.col("QtMercadoria") > 0, F.lit(1)).otherwise(F.lit(0))
    )
    .withColumn("Canal", F.lit("OFFLINE"))
)

print(f"‚úÖ Registros finais OFFLINE: {vendas_offline_final_df.count()}")

# Mostrar amostra dos dados
print("üìã Amostra dos dados OFFLINE:")
# display(vendas_offline_final_df.limit(5))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Carregamento de Vendas Online (Canais Digitais)

# COMMAND ----------

print(f"üåê Processando vendas ONLINE de {data_inicio_int} at√© {hoje_int}")

# üíæ Cache Strategy: Reutilizando vendas_nao_rateadas_df j√° em cache
# Carregar tabela de vendas rateadas (online)
vendas_rateadas_online_df = (
    spark.table("app_venda.vendafaturadarateada")
    .filter(F.col("NmEstadoMercadoria") != '1 - SALDO')
    .filter(F.col("NmTipoNegocio") != 'LOJA FISICA')  # Excluir loja f√≠sica
    .filter(
        F.col("DtAprovacao").between(data_inicio_int, hoje_int)
        & (F.col("VrOperacao") >= 0)
        & (F.col("VrCustoContabilFilialSku") >= 0)
    )
)

# Aplicar sample se configurado para desenvolvimento
if USAR_SAMPLES:
    print(f"üî¨ Aplicando sample de {SAMPLE_SIZE:,} registros ONLINE para desenvolvimento...")
    vendas_rateadas_online_df = vendas_rateadas_online_df.sample(fraction=0.1, seed=42).limit(SAMPLE_SIZE)

vendas_rateadas_online_df = vendas_rateadas_online_df.cache()

print(f"üìä Registros rateados ONLINE carregados: {vendas_rateadas_online_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Unifica√ß√£o dos Dados Online

# COMMAND ----------

# Unificar dados e aplicar transforma√ß√µes
vendas_online_unificadas_df = (
    vendas_rateadas_online_df
    .join(vendas_nao_rateadas_df.select("ChaveFatos", "QtMercadoria"), on="ChaveFatos")
    .withColumn(
        "year_month",
        F.date_format(
            F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
            "yyyyMM"
        ).cast("int")
    )
    .withColumnRenamed("CdFilialEmissao", "CdFilial")
    .withColumn(
        "DtAtual",
        F.date_format(
            F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
            "yyyy-MM-dd"
        )
    )
).cache()

print(f"üîó Registros ap√≥s join ONLINE: {vendas_online_unificadas_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Agrega√ß√£o das Vendas Online

# COMMAND ----------

# Agregar por filial, SKU e data
vendas_online_agregadas_df = (
    vendas_online_unificadas_df.groupBy(
        "DtAtual",
        "year_month",
        "CdSkuLoja",
        "CdFilial",
    )
    .agg(
        F.sum("VrOperacao").alias("Receita"),
        F.sum("QtMercadoria").alias("QtMercadoria"),
        F.sum("VrCustoContabilFilialSku").alias("Custo")
    )
)

print(f"üìä Registros ap√≥s agrega√ß√£o ONLINE: {vendas_online_agregadas_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Cria√ß√£o da Grade Completa Online

# COMMAND ----------

# Conjunto de chaves (Filial x SKU) para online
chaves_filial_sku_online_df = (
    vendas_online_unificadas_df
    .select("CdFilial", "CdSkuLoja")
    .dropDuplicates()
)

print(f"üîë Chaves √∫nicas ONLINE (Filial x SKU): {chaves_filial_sku_online_df.count()}")

# Grade completa (Data x Filial x SKU)
grade_completa_online_df = calendario_df.crossJoin(chaves_filial_sku_online_df)

# Agregado com data como DateType para join
vendas_online_agregadas_com_data_df = vendas_online_agregadas_df.withColumn("DtAtual_date", F.to_date("DtAtual"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Aplica√ß√£o da Grade Completa Online

# COMMAND ----------

# Left join + zeros onde n√£o houver venda
vendas_online_final_df = (
    grade_completa_online_df.join(
        vendas_online_agregadas_com_data_df,
        on=["DtAtual_date", "CdSkuLoja", "CdFilial"],
        how="left"
    )
    .withColumn("Receita", F.coalesce(F.col("Receita"), F.lit(0.0)))
        .withColumn("QtMercadoria", F.coalesce(F.col("QtMercadoria"), F.lit(0.0)))
        .withColumn("year_month", F.date_format(F.col("DtAtual_date"), "yyyyMM").cast("int"))
    .withColumn("DtAtual", F.date_format(F.col("DtAtual_date"), "yyyy-MM-dd"))
        .withColumnRenamed("CdSkuLoja", "CdSku")
    .select("DtAtual", "year_month", "CdFilial", "CdSku", "Receita", "QtMercadoria")
    .withColumn(
        "TeveVenda",
        F.when(F.col("QtMercadoria") > 0, F.lit(1)).otherwise(F.lit(0))
    )
    .withColumn("Canal", F.lit("ONLINE"))
)

print(f"‚úÖ Registros finais ONLINE: {vendas_online_final_df.count()}")

# Mostrar amostra dos dados
print("üìã Amostra dos dados ONLINE:")
# display(vendas_online_final_df.limit(5))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Consolida√ß√£o de Vendas Online e Offline com Outer Join

# COMMAND ----------

print("üîÑ Consolidando vendas ONLINE e OFFLINE com outer join...")

# üíæ Cache Strategy: DataFrame final consolidado em cache para m√∫ltiplas opera√ß√µes
# Valida√ß√£o de dados
if vendas_offline_final_df.count() == 0 and vendas_online_final_df.count() == 0:
    raise ValueError("Ambos os DataFrames de vendas est√£o vazios")

# Fazer outer join para garantir que todas as filiais apare√ßam em ambos os canais
# Primeiro, vamos preparar os DataFrames para o join
vendas_offline_preparadas_df = (
    vendas_offline_final_df
    .withColumnRenamed("Receita", "Receita_OFF")
    .withColumnRenamed("QtMercadoria", "QtMercadoria_OFF")
    .withColumnRenamed("TeveVenda", "TeveVenda_OFF")
    .drop("Canal")  # Remover coluna Canal pois n√£o precisamos mais
)

vendas_online_preparadas_df = (
    vendas_online_final_df
    .withColumnRenamed("Receita", "Receita_ON")
    .withColumnRenamed("QtMercadoria", "QtMercadoria_ON")
    .withColumnRenamed("TeveVenda", "TeveVenda_ON")
    .drop("Canal")  # Remover coluna Canal pois n√£o precisamos mais
)

# Outer join para garantir todas as combina√ß√µes
vendas_consolidadas_temp_df = (
    vendas_offline_preparadas_df
    .join(
        vendas_online_preparadas_df,
        on=["DtAtual", "year_month", "CdFilial", "CdSku"],
        how="outer"
    )
)

# Preencher valores nulos com zeros para garantir dados completos
vendas_consolidadas_df = (
    vendas_consolidadas_temp_df
    .fillna(0, subset=[
        "Receita_OFF", "QtMercadoria_OFF", "TeveVenda_OFF",
        "Receita_ON", "QtMercadoria_ON", "TeveVenda_ON"
    ])
    # Calcular totais consolidados
    .withColumn("Receita", F.col("Receita_OFF") + F.col("Receita_ON"))
    .withColumn("QtMercadoria", F.col("QtMercadoria_OFF") + F.col("QtMercadoria_ON"))
    .withColumn("TeveVenda", F.col("TeveVenda_OFF") + F.col("TeveVenda_ON"))
    # Selecionar colunas finais
    .select(
        "DtAtual", "year_month", "CdFilial", "CdSku",
        "Receita_OFF", "QtMercadoria_OFF", "TeveVenda_OFF",
        "Receita_ON", "QtMercadoria_ON", "TeveVenda_ON",
        "Receita", "QtMercadoria", "TeveVenda"
    )
).cache()

print(f"üìä Total de registros consolidados: {vendas_consolidadas_df.count()}")

# Mostrar estat√≠sticas de filiais por canal
print("üìà Estat√≠sticas de filiais por canal:")
filiais_offline = vendas_offline_final_df.select("CdFilial").distinct().count()
filiais_online = vendas_online_final_df.select("CdFilial").distinct().count()
filiais_consolidadas = vendas_consolidadas_df.select("CdFilial").distinct().count()

print(f"  ‚Ä¢ Filiais com vendas OFFLINE: {filiais_offline}")
print(f"  ‚Ä¢ Filiais com vendas ONLINE: {filiais_online}")
print(f"  ‚Ä¢ Filiais consolidadas: {filiais_consolidadas}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## An√°lise Estat√≠stica Consolidada

# COMMAND ----------

# Mostrar estat√≠sticas consolidadas
print("üìà Estat√≠sticas consolidadas:")
estatisticas_consolidadas_df = vendas_consolidadas_df.agg(
    F.count("*").alias("Total_Registros"),
    F.sum("Receita").alias("Receita_Total"),
    F.sum("QtMercadoria").alias("Quantidade_Total"),
    F.sum("TeveVenda").alias("Dias_Com_Venda"),
    F.countDistinct("CdFilial").alias("Filiais_Unicas"),
    F.countDistinct("CdSku").alias("SKUs_Unicos"),
    F.sum("Receita_OFF").alias("Receita_OFF"),
    F.sum("Receita_ON").alias("Receita_ON"),
    F.sum("QtMercadoria_OFF").alias("Quantidade_OFF"),
    F.sum("QtMercadoria_ON").alias("Quantidade_ON")
)

# display(estatisticas_consolidadas_df)

# Mostrar estat√≠sticas por filial (top 10)
print("üìä Top 10 filiais por receita total:")
top_filiais_df = (
    vendas_consolidadas_df
    .groupBy("CdFilial")
    .agg(
        F.sum("Receita").alias("Receita_Total"),
        F.sum("Receita_OFF").alias("Receita_OFF"),
        F.sum("Receita_ON").alias("Receita_ON"),
        F.sum("QtMercadoria").alias("Quantidade_Total"),
        F.sum("TeveVenda").alias("Dias_Com_Venda")
    )
    .orderBy(F.desc("Receita_Total"))
    .limit(10)
)

# display(top_filiais_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Adi√ß√£o de Metadados de Processamento

# COMMAND ----------

# Adicionar metadados de processamento
vendas_com_metadados_df = vendas_consolidadas_df.withColumn(
    "DataHoraProcessamento", 
    F.current_timestamp()
).withColumn(
    "DataProcessamento",
    F.current_date()
).withColumn(
    "FonteDados",
    F.lit("app_venda.vendafaturadarateada + vendafaturadanaorateada")
).withColumn(
    "VersaoProcessamento",
    F.lit("1.0")
)

print(f"üìä Registros com metadados: {vendas_com_metadados_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Salvamento na Camada Bronze

# COMMAND ----------

print(f"üíæ Salvando tabela {TABELA_BRONZE_VENDAS} no modo overwrite...")

try:
    # Salvar na camada Bronze
    vendas_com_metadados_df.write \
        .format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .saveAsTable(TABELA_BRONZE_VENDAS)
    
    print(f"‚úÖ Tabela {TABELA_BRONZE_VENDAS} salva com sucesso!")
    print(f"üìä Registros salvos: {vendas_com_metadados_df.count()}")

except Exception as e:
    print(f"‚ùå Erro ao salvar tabela {TABELA_BRONZE_VENDAS}: {str(e)}")
    raise
finally:
    # Limpar cache para liberar mem√≥ria
    print("üßπ Limpando cache para liberar mem√≥ria...")
    vendas_rateadas_offline_df.unpersist()
    vendas_nao_rateadas_df.unpersist()
    vendas_offline_unificadas_df.unpersist()
    vendas_rateadas_online_df.unpersist()
    vendas_online_unificadas_df.unpersist()
    vendas_consolidadas_df.unpersist()
    print("‚úÖ Cache limpo com sucesso!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Valida√ß√£o da Tabela Salva

# COMMAND ----------

# Mostrar schema da tabela salva
print("üìã Schema da tabela salva:")
spark.table(TABELA_BRONZE_VENDAS).printSchema()

# Mostrar amostra dos dados salvos
print("üìã Amostra dos dados salvos:")
# display(spark.table(TABELA_BRONZE_VENDAS).limit(5))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Resumo Final do Processamento

# COMMAND ----------

print("üéâ Processamento de vendas Bronze conclu√≠do com sucesso!")
print("=" * 80)
print("üìä RESUMO DO PROCESSAMENTO:")
print(f"  ‚Ä¢ Per√≠odo processado: {data_inicio_str} at√© {hoje_str}")
print(f"  ‚Ä¢ Dias de hist√≥rico: {dias_retrocesso}")
print(f"  ‚Ä¢ Registros offline: {vendas_offline_final_df.count():,}")
print(f"  ‚Ä¢ Registros online: {vendas_online_final_df.count():,}")
print(f"  ‚Ä¢ Total consolidado: {vendas_consolidadas_df.count():,}")
print(f"  ‚Ä¢ Filiais √∫nicas: {filiais_consolidadas}")
print(f"  ‚Ä¢ Tabela de destino: {TABELA_BRONZE_VENDAS}")
print("=" * 80)
print("‚úÖ PROCESSAMENTO CONCLU√çDO COM SUCESSO!")
print("üîÑ Outer join aplicado para garantir todas as combina√ß√µes filial-SKU")
print("üî¢ Valores nulos preenchidos com zeros")
print("üìä Estrutura: SKU x Loja x Dia com colunas separadas por canal")
print("üìà Colunas: Receita_OFF, Receita_ON, QtMercadoria_OFF, QtMercadoria_ON, etc.")
