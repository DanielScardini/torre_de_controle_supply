#%% [markdown]
# ğŸ“Š Processamento de Vendas - Camada Bronze
#
# Este notebook processa dados de vendas online e offline para a camada Bronze,
# seguindo o padrÃ£o Medallion Architecture e as melhores prÃ¡ticas Python.
#
# <details>
# <summary><b>ğŸ¯ Objetivos do Projeto</b></summary>
#
# - Processar vendas online e offline de forma unificada
# - Agregar dados por filial, SKU e data
# - Criar grade completa de vendas com zeros
# - Salvar na camada Bronze com metadados
# - Implementar timezone SÃ£o Paulo (GMT-3)
# - Seguir padrÃµes de qualidade de cÃ³digo
#
# </details>
#
# <details>
# <summary><b>ğŸ“‹ Funcionalidades Principais</b></summary>
#
# - **Processamento Offline**: Vendas de loja fÃ­sica
# - **Processamento Online**: Vendas de canais digitais
# - **ConsolidaÃ§Ã£o**: UniÃ£o de ambos os canais
# - **Grade Completa**: Todas as combinaÃ§Ãµes filial Ã— SKU Ã— data
# - **Metadados**: DataHoraProcessamento, FonteDados, VersaoProcessamento
# - **ValidaÃ§Ãµes**: Tratamento de erros e dados inconsistentes
#
# </details>
#
# ---
#
# **Autor**: Torre de Controle Supply Chain - 2024

#%% [markdown]
# ## 1. Setup e Imports
#
# <details>
# <summary><b>ğŸ“¦ Bibliotecas Utilizadas</b></summary>
#
# - **pyspark**: Processamento distribuÃ­do de dados
# - **datetime**: ManipulaÃ§Ã£o de datas e horÃ¡rios
# - **typing**: AnotaÃ§Ãµes de tipo para melhor documentaÃ§Ã£o
# - **pytz**: Tratamento de timezones
#
# </details>

#%%
# Import necessary libraries
from datetime import datetime, timedelta, date
from typing import Optional, Union

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pytz import timezone

print("âœ… Bibliotecas importadas com sucesso!")

#%% [markdown]
# ## 2. ConfiguraÃ§Ãµes Globais
#
# <details>
# <summary><b>âš™ï¸ ConfiguraÃ§Ãµes do Sistema</b></summary>
#
# - **Tabela de Destino**: `databox.bcg_comum.supply_bronze_vendas_90d_on_off`
# - **Timezone**: SÃ£o Paulo (GMT-3)
# - **PerÃ­odo**: Ãšltimos 90 dias (configurÃ¡vel)
# - **Modo de Salvamento**: Overwrite por padrÃ£o
#
# </details>

#%%
# =============================================================================
# CONFIGURAÃ‡Ã•ES GLOBAIS
# =============================================================================

# Nome da tabela de destino na camada Bronze
TABELA_BRONZE_VENDAS: str = "databox.bcg_comum.supply_bronze_vendas_90d_on_off"

# Timezone SÃ£o Paulo (GMT-3)
TIMEZONE_SP = timezone('America/Sao_Paulo')

# InicializaÃ§Ã£o do Spark
spark = SparkSession.builder.appName("vendas_bronze").getOrCreate()

# Data de processamento (ontem)
hoje = datetime.now(TIMEZONE_SP) - timedelta(days=1)
hoje_str = hoje.strftime("%Y-%m-%d")
hoje_int = int(hoje.strftime("%Y%m%d"))

print(f"ğŸ“… Data de processamento: {hoje}")
print(f"ğŸ“ Data string: {hoje_str}")
print(f"ğŸ”¢ Data int: {hoje_int}")
print(f"ğŸŒ Timezone: {TIMEZONE_SP}")

#%% [markdown]
# ## 3. FunÃ§Ã£o de CÃ¡lculo de Data de InÃ­cio
#
# <details>
# <summary><b>ğŸ“Š LÃ³gica de CÃ¡lculo</b></summary>
#
# Esta funÃ§Ã£o calcula a data de inÃ­cio baseada nos Ãºltimos N dias:
# - **PadrÃ£o**: 90 dias de retrocesso
# - **Flexibilidade**: Permite ajustar o perÃ­odo
# - **Timezone**: Considera timezone SÃ£o Paulo
# - **ValidaÃ§Ã£o**: Verifica se dias_retrocesso Ã© positivo
#
# </details>

#%%
def get_data_inicio(
    hoje: Optional[Union[datetime, date]] = None, 
    dias_retrocesso: int = 90
) -> datetime:
    """
    Retorna datetime de inÃ­cio baseado nos Ãºltimos N dias.
    
    Args:
        hoje: Data de referÃªncia (padrÃ£o: hoje)
        dias_retrocesso: NÃºmero de dias para retroceder (padrÃ£o: 90 dias)
        
    Returns:
        datetime: Data de inÃ­cio no timezone SÃ£o Paulo
        
    Raises:
        ValueError: Se dias_retrocesso for negativo
    """
    if dias_retrocesso < 0:
        raise ValueError("dias_retrocesso deve ser positivo")
    
    if hoje is None:
        hoje_dt = datetime.now(TIMEZONE_SP)
    elif isinstance(hoje, datetime):
        hoje_dt = hoje
    else:
        hoje_dt = datetime.combine(hoje, datetime.min.time())
        hoje_dt = TIMEZONE_SP.localize(hoje_dt)
    
    data_inicio = hoje_dt - timedelta(days=dias_retrocesso)
    
    print(f"ğŸ“Š Data de inÃ­cio calculada: {data_inicio}")
    print(f"â° Dias de retrocesso: {dias_retrocesso}")
    
    return data_inicio

# Executar funÃ§Ã£o e mostrar resultado
data_inicio = get_data_inicio()
data_inicio_int = int(data_inicio.strftime("%Y%m%d"))
print(f"ğŸ“… Data inÃ­cio: {data_inicio}")
print(f"ğŸ”¢ Data inÃ­cio int: {data_inicio_int}")

# Mostrar DataFrame de exemplo
df_exemplo = spark.range(1).select(
    F.lit(data_inicio).alias("data_inicio"),
    F.lit(data_inicio_int).alias("data_inicio_int"),
    F.lit(90).alias("dias_retrocesso")
)
df_exemplo.show()

#%% [markdown]
# ## 4. Processamento de Vendas Offline
#
# <details>
# <summary><b>ğŸª LÃ³gica de Processamento Offline</b></summary>
#
# Esta seÃ§Ã£o processa vendas de loja fÃ­sica:
# - **Fonte**: Tabela `app_venda.vendafaturadarateada`
# - **Filtros**: Apenas loja fÃ­sica, valores positivos
# - **AgregaÃ§Ã£o**: Por filial, SKU e data
# - **Grade Completa**: Todas as combinaÃ§Ãµes com zeros
# - **Canal**: Identificado como "OFFLINE"
#
# </details>

#%%
def get_vendas_offline(
    spark: SparkSession,
    start_date: int = data_inicio_int,
    end_date: int = hoje_int,
) -> DataFrame:
    """
    Processa vendas offline (loja fÃ­sica) da tabela vendafaturadarateada.
    
    Args:
        spark: SessÃ£o do Spark
        start_date: Data de inÃ­cio no formato YYYYMMDD
        end_date: Data de fim no formato YYYYMMDD
        
    Returns:
        DataFrame com vendas offline agregadas por filial, SKU e data
        
    Raises:
        ValueError: Se start_date > end_date
    """
    if start_date > end_date:
        raise ValueError("start_date deve ser menor ou igual a end_date")
    
    print(f"ğŸª Processando vendas OFFLINE de {start_date} atÃ© {end_date}")
    
    # Carregar tabela de vendas rateadas (offline)
    df_rateada = (
        spark.table("app_venda.vendafaturadarateada")
        .filter(F.col("NmEstadoMercadoria") != '1 - SALDO')
        .filter(F.col("NmTipoNegocio") == 'LOJA FISICA')
        .filter(
            F.col("DtAprovacao").between(start_date, end_date)
            & (F.col("VrOperacao") >= 0)
            & (F.col("VrCustoContabilFilialSku") >= 0)
        )
    )
    
    print(f"ğŸ“Š Registros rateados carregados: {df_rateada.count()}")
    
    # Carregar tabela de vendas nÃ£o rateadas para quantidade
    df_nao_rateada = (
        spark.table("app_venda.vendafaturadanaorateada")
        .filter(F.col("QtMercadoria") >= 0)
    )
    
    print(f"ğŸ“ˆ Registros nÃ£o rateados carregados: {df_nao_rateada.count()}")
    
    # Unificar dados e aplicar transformaÃ§Ãµes
    df = (
        df_rateada
        .join(df_nao_rateada.select("ChaveFatos", "QtMercadoria"), on="ChaveFatos")
        .withColumn(
            "year_month",
            F.date_format(
                F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
                "yyyyMM"
            ).cast("int")
        )
        .withColumnRenamed("CdFilialVenda", "CdFilial")
        .withColumn(
            "DtAtual",
            F.date_format(
                F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
                "yyyy-MM-dd"
            )
        )
    )
    
    print(f"ğŸ”— Registros apÃ³s join: {df.count()}")
    
    # Agregar por filial, SKU e data
    df_agg = (
        df.groupBy(
            "DtAtual",
            "year_month",
            "CdSkuLoja",
            "CdFilial",
        )
        .agg(
            F.sum("VrOperacao").alias("Receita"),
            F.sum("QtMercadoria").alias("QtMercadoria"),
            F.sum("VrCustoContabilFilialSku").alias("Custo")
        )
    )
    
    print(f"ğŸ“Š Registros apÃ³s agregaÃ§Ã£o: {df_agg.count()}")
    
    # Criar grade completa de datas
    cal = (
        spark.range(1)
        .select(
            F.explode(
                F.sequence(
                    F.to_date(F.lit(str(start_date)), "yyyyMMdd"),
                    F.to_date(F.lit(str(end_date)), "yyyyMMdd"),
                    F.expr("interval 1 day")
                )
            ).alias("DtAtual_date")
        )
    )
    
    # Conjunto de chaves (Filial x SKU) para loja fÃ­sica
    keys = (
        df
        .select("CdFilial", "CdSkuLoja")
        .dropDuplicates()
    )
    
    print(f"ğŸ”‘ Chaves Ãºnicas (Filial x SKU): {keys.count()}")
    
    # Grade completa (Data x Filial x SKU)
    grade = cal.crossJoin(keys)
    
    # Agregado com data como DateType para join
    df_agg_d = df_agg.withColumn("DtAtual_date", F.to_date("DtAtual"))
    
    # Left join + zeros onde nÃ£o houver venda
    result = (
        grade.join(
            df_agg_d,
            on=["DtAtual_date", "CdSkuLoja", "CdFilial"],
            how="left"
        )
        .withColumn("Receita", F.coalesce(F.col("Receita"), F.lit(0.0)))
        .withColumn("QtMercadoria", F.coalesce(F.col("QtMercadoria"), F.lit(0.0)))
        .withColumn("year_month", F.date_format(F.col("DtAtual_date"), "yyyyMM").cast("int"))
        .withColumn("DtAtual", F.date_format(F.col("DtAtual_date"), "yyyy-MM-dd"))
        .withColumnRenamed("CdSkuLoja", "CdSku")
        .select("DtAtual", "year_month", "CdFilial", "CdSku", "Receita", "QtMercadoria")
        .withColumn(
            "TeveVenda",
            F.when(F.col("QtMercadoria") > 0, F.lit(1)).otherwise(F.lit(0))
        )
        .withColumn("Canal", F.lit("OFFLINE"))
    )
    
    print(f"âœ… Registros finais OFFLINE: {result.count()}")
    
    # Mostrar amostra dos dados
    print("ğŸ“‹ Amostra dos dados OFFLINE:")
    result.show(5)
    
    return result

# Executar funÃ§Ã£o de vendas offline
vendas_offline_df = get_vendas_offline(spark)
print(f"ğŸª DataFrame vendas offline criado com {vendas_offline_df.count()} registros")

#%% [markdown]
# ## 5. Processamento de Vendas Online
#
# <details>
# <summary><b>ğŸŒ LÃ³gica de Processamento Online</b></summary>
#
# Esta seÃ§Ã£o processa vendas de canais digitais:
# - **Fonte**: Tabela `app_venda.vendafaturadarateada`
# - **Filtros**: Exclui loja fÃ­sica, valores positivos
# - **AgregaÃ§Ã£o**: Por filial, SKU e data
# - **Grade Completa**: Todas as combinaÃ§Ãµes com zeros
# - **Canal**: Identificado como "ONLINE"
#
# </details>

#%%
def get_vendas_online(
    spark: SparkSession,
    start_date: int = data_inicio_int,
    end_date: int = hoje_int,
) -> DataFrame:
    """
    Processa vendas online da tabela vendafaturadarateada.
    
    Args:
        spark: SessÃ£o do Spark
        start_date: Data de inÃ­cio no formato YYYYMMDD
        end_date: Data de fim no formato YYYYMMDD
        
    Returns:
        DataFrame com vendas online agregadas por filial, SKU e data
        
    Raises:
        ValueError: Se start_date > end_date
    """
    if start_date > end_date:
        raise ValueError("start_date deve ser menor ou igual a end_date")
    
    print(f"ğŸŒ Processando vendas ONLINE de {start_date} atÃ© {end_date}")
    
    # Carregar tabela de vendas rateadas (online)
    df_rateada = (
        spark.table("app_venda.vendafaturadarateada")
        .filter(F.col("NmEstadoMercadoria") != '1 - SALDO')
        .filter(F.col("NmTipoNegocio") != 'LOJA FISICA')  # Excluir loja fÃ­sica
        .filter(
            F.col("DtAprovacao").between(start_date, end_date)
            & (F.col("VrOperacao") >= 0)
            & (F.col("VrCustoContabilFilialSku") >= 0)
        )
    )
    
    print(f"ğŸ“Š Registros rateados ONLINE carregados: {df_rateada.count()}")
    
    # Carregar tabela de vendas nÃ£o rateadas para quantidade
    df_nao_rateada = (
        spark.table("app_venda.vendafaturadanaorateada")
        .filter(F.col("QtMercadoria") >= 0)
    )
    
    # Unificar dados e aplicar transformaÃ§Ãµes
    df = (
        df_rateada
        .join(df_nao_rateada.select("ChaveFatos", "QtMercadoria"), on="ChaveFatos")
        .withColumn(
            "year_month",
            F.date_format(
                F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
                "yyyyMM"
            ).cast("int")
        )
        .withColumnRenamed("CdFilialVenda", "CdFilial")
        .withColumn(
            "DtAtual",
            F.date_format(
                F.to_date(F.col("DtAprovacao").cast("string"), "yyyyMMdd"), 
                "yyyy-MM-dd"
            )
        )
    )
    
    print(f"ğŸ”— Registros apÃ³s join ONLINE: {df.count()}")
    
    # Agregar por filial, SKU e data
    df_agg = (
        df.groupBy(
            "DtAtual",
            "year_month",
            "CdSkuLoja",
            "CdFilial",
        )
        .agg(
            F.sum("VrOperacao").alias("Receita"),
            F.sum("QtMercadoria").alias("QtMercadoria"),
            F.sum("VrCustoContabilFilialSku").alias("Custo")
        )
    )
    
    print(f"ğŸ“Š Registros apÃ³s agregaÃ§Ã£o ONLINE: {df_agg.count()}")
    
    # Criar grade completa de datas
    cal = (
        spark.range(1)
        .select(
            F.explode(
                F.sequence(
                    F.to_date(F.lit(str(start_date)), "yyyyMMdd"),
                    F.to_date(F.lit(str(end_date)), "yyyyMMdd"),
                    F.expr("interval 1 day")
                )
            ).alias("DtAtual_date")
        )
    )
    
    # Conjunto de chaves (Filial x SKU) para online
    keys = (
        df
        .select("CdFilial", "CdSkuLoja")
        .dropDuplicates()
    )
    
    print(f"ğŸ”‘ Chaves Ãºnicas ONLINE (Filial x SKU): {keys.count()}")
    
    # Grade completa (Data x Filial x SKU)
    grade = cal.crossJoin(keys)
    
    # Agregado com data como DateType para join
    df_agg_d = df_agg.withColumn("DtAtual_date", F.to_date("DtAtual"))
    
    # Left join + zeros onde nÃ£o houver venda
    result = (
        grade.join(
            df_agg_d,
            on=["DtAtual_date", "CdSkuLoja", "CdFilial"],
            how="left"
        )
        .withColumn("Receita", F.coalesce(F.col("Receita"), F.lit(0.0)))
        .withColumn("QtMercadoria", F.coalesce(F.col("QtMercadoria"), F.lit(0.0)))
        .withColumn("year_month", F.date_format(F.col("DtAtual_date"), "yyyyMM").cast("int"))
        .withColumn("DtAtual", F.date_format(F.col("DtAtual_date"), "yyyy-MM-dd"))
        .withColumnRenamed("CdSkuLoja", "CdSku")
        .select("DtAtual", "year_month", "CdFilial", "CdSku", "Receita", "QtMercadoria")
        .withColumn(
            "TeveVenda",
            F.when(F.col("QtMercadoria") > 0, F.lit(1)).otherwise(F.lit(0))
        )
        .withColumn("Canal", F.lit("ONLINE"))
    )
    
    print(f"âœ… Registros finais ONLINE: {result.count()}")
    
    # Mostrar amostra dos dados
    print("ğŸ“‹ Amostra dos dados ONLINE:")
    result.show(5)
    
    return result

# Executar funÃ§Ã£o de vendas online
vendas_online_df = get_vendas_online(spark)
print(f"ğŸŒ DataFrame vendas online criado com {vendas_online_df.count()} registros")

#%% [markdown]
# ## 6. ConsolidaÃ§Ã£o de Vendas Online e Offline
#
# <details>
# <summary><b>ğŸ”„ LÃ³gica de ConsolidaÃ§Ã£o</b></summary>
#
# Esta seÃ§Ã£o unifica os dados de ambos os canais:
# - **UniÃ£o**: Combina DataFrames online e offline
# - **EstatÃ­sticas**: Mostra resumo por canal
# - **ValidaÃ§Ã£o**: Verifica se hÃ¡ dados para processar
# - **Amostra**: Exibe dados consolidados para verificaÃ§Ã£o
#
# </details>

#%%
def consolidar_vendas_online_offline(
    vendas_offline_df: DataFrame,
    vendas_online_df: DataFrame
) -> DataFrame:
    """
    Consolida vendas online e offline em um Ãºnico DataFrame.
    
    Args:
        vendas_offline_df: DataFrame com vendas offline
        vendas_online_df: DataFrame com vendas online
        
    Returns:
        DataFrame consolidado com vendas de ambos os canais
        
    Raises:
        ValueError: Se os DataFrames estiverem vazios
    """
    if vendas_offline_df.count() == 0 and vendas_online_df.count() == 0:
        raise ValueError("Ambos os DataFrames de vendas estÃ£o vazios")
    
    print("ğŸ”„ Consolidando vendas ONLINE e OFFLINE...")
    
    # Unir os DataFrames
    vendas_consolidadas = vendas_offline_df.union(vendas_online_df)
    
    print(f"ğŸ“Š Total de registros consolidados: {vendas_consolidadas.count()}")
    
    # Mostrar estatÃ­sticas por canal
    print("ğŸ“ˆ EstatÃ­sticas por canal:")
    vendas_consolidadas.groupBy("Canal").agg(
        F.count("*").alias("Total_Registros"),
        F.sum("Receita").alias("Receita_Total"),
        F.sum("QtMercadoria").alias("Quantidade_Total"),
        F.sum("TeveVenda").alias("Dias_Com_Venda")
    ).show()
    
    # Mostrar amostra dos dados consolidados
    print("ğŸ“‹ Amostra dos dados consolidados:")
    vendas_consolidadas.show(10)
    
    return vendas_consolidadas

# Executar consolidaÃ§Ã£o
vendas_consolidadas_df = consolidar_vendas_online_offline(vendas_offline_df, vendas_online_df)
print(f"ğŸ”„ DataFrame consolidado criado com {vendas_consolidadas_df.count()} registros")

#%% [markdown]
# ## 7. Salvamento na Camada Bronze
#
# <details>
# <summary><b>ğŸ’¾ LÃ³gica de Salvamento</b></summary>
#
# Esta seÃ§Ã£o salva os dados processados na camada Bronze:
# - **Metadados**: DataHoraProcessamento, FonteDados, VersaoProcessamento
# - **Timezone**: GMT-3 SÃ£o Paulo para DataHoraProcessamento
# - **Modo**: Overwrite por padrÃ£o (configurÃ¡vel)
# - **ValidaÃ§Ã£o**: Verifica sucesso do salvamento
# - **Schema**: Mostra estrutura da tabela criada
#
# </details>

#%%
def salvar_tabela_bronze(
    df: DataFrame,
    nome_tabela: str = TABELA_BRONZE_VENDAS,
    modo: str = "overwrite"
) -> bool:
    """
    Salva DataFrame na camada Bronze com metadados de processamento.
    
    Args:
        df: DataFrame para salvar
        nome_tabela: Nome da tabela de destino
        modo: Modo de salvamento (overwrite, append)
        
    Returns:
        bool: True se salvamento foi bem-sucedido
        
    Raises:
        ValueError: Se modo nÃ£o for vÃ¡lido
    """
    if modo not in ["overwrite", "append"]:
        raise ValueError("modo deve ser 'overwrite' ou 'append'")
    
    try:
        print(f"ğŸ’¾ Salvando tabela {nome_tabela} no modo {modo}...")
        
        # Adicionar metadados de processamento
        df_com_metadados = df.withColumn(
            "DataHoraProcessamento", 
            F.current_timestamp()
        ).withColumn(
            "DataProcessamento",
            F.current_date()
        ).withColumn(
            "FonteDados",
            F.lit("app_venda.vendafaturadarateada + vendafaturadanaorateada")
        ).withColumn(
            "VersaoProcessamento",
            F.lit("1.0")
        )
        
        # Salvar na camada Bronze
        df_com_metadados.write \
            .format("delta") \
            .mode(modo) \
            .option("overwriteSchema", "true") \
            .saveAsTable(nome_tabela)
        
        print(f"âœ… Tabela {nome_tabela} salva com sucesso!")
        print(f"ğŸ“Š Registros salvos: {df_com_metadados.count()}")
        
        # Mostrar schema da tabela salva
        print("ğŸ“‹ Schema da tabela salva:")
        spark.table(nome_tabela).printSchema()
        
        # Mostrar amostra dos dados salvos
        print("ğŸ“‹ Amostra dos dados salvos:")
        spark.table(nome_tabela).show(5)
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro ao salvar tabela {nome_tabela}: {str(e)}")
        return False

# Executar salvamento
sucesso = salvar_tabela_bronze(vendas_consolidadas_df)
if sucesso:
    print("ğŸ‰ Processamento de vendas Bronze concluÃ­do com sucesso!")
else:
    print("ğŸ’¥ Falha no processamento de vendas Bronze!")

#%% [markdown]
# ## 8. ConclusÃµes e Resumo Final
#
# <details>
# <summary><b>ğŸ” Principais Resultados</b></summary>
#
# ### âœ… Processamento ConcluÃ­do
# - **Vendas Offline**: Processadas com sucesso
# - **Vendas Online**: Processadas com sucesso
# - **ConsolidaÃ§Ã£o**: Dados unificados
# - **Salvamento**: Tabela Bronze criada
#
# ### ğŸ“Š CaracterÃ­sticas da Tabela
# - **Nome**: `databox.bcg_comum.supply_bronze_vendas_90d_on_off`
# - **PerÃ­odo**: Ãšltimos 90 dias (configurÃ¡vel)
# - **Grade Completa**: Todas as combinaÃ§Ãµes filial Ã— SKU Ã— data
# - **Canais**: ONLINE e OFFLINE identificados
# - **Metadados**: DataHoraProcessamento, FonteDados, VersaoProcessamento
#
# ### ğŸ¯ PrÃ³ximos Passos
# - **Camada Silver**: Processar dados limpos e conformados
# - **Camada Gold**: Criar agregaÃ§Ãµes para dashboards
# - **ValidaÃ§Ã£o**: Implementar testes de qualidade
# - **Monitoramento**: Configurar alertas de processamento
#
# </details>
#
# ---
#
# ### ğŸ“ Resumo Executivo
#
# Este notebook demonstrou um fluxo completo de processamento de dados incluindo:
# - âœ… **Carregamento**: Dados de vendas online e offline
# - âœ… **TransformaÃ§Ã£o**: AgregaÃ§Ã£o e criaÃ§Ã£o de grade completa
# - âœ… **ConsolidaÃ§Ã£o**: UniÃ£o de ambos os canais
# - âœ… **Salvamento**: PersistÃªncia na camada Bronze com metadados
# - âœ… **ValidaÃ§Ã£o**: VerificaÃ§Ã£o de integridade dos dados
# - âœ… **DocumentaÃ§Ã£o**: CÃ³digo bem documentado e organizado
#
# **Status**: âœ… **CONCLUÃDO COM SUCESSO**